---
title: "Communicative modes in parent—child conversations"


bibliography: child-discourse.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
   \author{Anonymous}
   
abstract:
  
keywords: >
  language input, language acquisition, child-directed speech, corpus analysis, word2vec
    
output: cogsci2016::cogsci_paper
final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo = F, warning = F, cache = F, 
                      message = F, sanitize = T)
# Note: to build, 
options(digits=2)
```

```{r, libraries}
library(png)
library(ggplot2)
library(here)
library(ggridges)
library(scales)
library(tidyboot)
library(xtable)
library(papaja)
library(lme4)
library(lmerTest)
library(ggthemes)
library(broom.mixed)
library(tidyverse)
library(grid)
library(feather)
library(reticulate)
use_condaenv("r-reticulate")

theme_set(theme_few())
```

```{r read-data}
childes_htmm_pdwz <- read_feather(here("data/childes_htmm_all_models"))
childes_htmm_phi <- read_csv(here("data/phi_vals_all_models.csv"))
dict <- read_csv(here("data/childes_gloss_word_dict.csv"))
```



Words are amenable to storage, tabulation, and manipulation using common computational tools. Perhaps due to their ease of use, words have received outsize attention in modeling language development.  

Studying children's communicative acts using a computational model also allows us to extract communicative patterns across many children with less a priori specification of what those patterns are. Traditionally, studies of communicative acts among children have brought frameworks from adult communication, such as Speech Act theory [@austin_how_1962, @searle_speech_1969] and Conversation Analysis [@sacks_simplest_1974], to bear on children's conversations. While concepts like 'directive', 'expressive', 'accusation' and 'justification' can be useful to characterize children's conversations, young children may not have the same communicative needs as adults—these may not be the right joints at which to carve. 

Here, we characterize children's growing repertoire of communicative acts using a Hidden Topic Markov Model. This model observes utterances produced by parents and children and attempts to infer common underlying processes—topics—that produced them.  

We first show that without top-down specification, this model extracts several communicative acts analogous to those observed in close case studies of children's communication. We then show that use of these acts has a developmental trajectory in line with findings from close studies of children's communication. ...

## Corpus

We use transcripts of conversations from the Child Language Data Exchange System (CHILDES), a database of child conversation corpora [@macwhinney_childes_2000]. These corpora predominantly record spontaneous conversations between children and their family members, often in the home. We filtered the transcripts to include only utterances spoken by the target child or their parents. Overall, our training data included 4,043 transcripts from 411 children three months to 60 months old. 

## Model

We use a Hidden Topic Markov Model [@gruber_hidden_2007] to extract communicative modes from parent—child conversations. Topic models represent documents as mixtures of topics, and topics as mixtures of words. For instance, a simple topic model trained on news articles may extract a topic whose distinctive words are "fire", "flood", and "aid" and another whose distinctive words are "speech", "legislation", and "administration". Based on its distribution of words, an article about politicians' provision of disaster relief may be correctly inferred to feature these two topics, among others. Intuitively, the goal of a topic model is to recover the underlying sources—topics—from which the words in a document spring. 

The Hidden Topic Markov Model (HTMM) differs from a simple topic model in that it takes into account the sequential utterance structure of a document, not just its static distribution of words. The HTMM assumes that words within an utterance are of the same topic, and that sequential utterances may be more likely to be of the same topic. It represents topic transitions between utterances in a coarse-grained way: either switch or stay. @gruber_hidden_2007 develop this model and use it to segment machine learning conference papers, showing that the model can distinguish instances of the word "support" in mathematical contexts (describing support vectors) from those in the context of acknowledgements. Typically, function words are removed from corpora before training topic models to aid detection of thematic content. Here, we aim to classify communicative modes underlying utterances rather than thematic topics. We expect function words to be diagnostic of these modes, so include them in our training data.

After training, the model produces a set of topics with associated probability distributions over words and a probability distribution over topics for each utterance in the corpus. The most distinctive words of each topic in our 15–topic model, as measured by difference between a word's probability within a topic and its probability in the corpus as a whole, are shown in Figure 1. 

```{r topic_table, results="asis", tab.env = "table"}
tab <- childes_htmm_phi %>%
  mutate(phi_diff = phi - avg_phi) %>%
  filter(model == "15") %>%
  group_by(topic) %>%
  arrange(desc(phi_diff)) %>%
  slice(1:10) %>%
  mutate(num = 1:10) %>%
  ungroup() %>%
  select(topic, num, word) %>%
  pivot_wider(names_from = topic, values_from = word)  %>% select(-num) %>%
  as_tibble() %>% xtable()

print(tab, type = "latex", comment = F, table.placement = "tb", floating = TRUE,
      floating.environment = "table*",
      include.rownames = FALSE)
```

##





```{r entropy-par-child}

childes_htmm_pdwz <- childes_htmm_pdwz %>%
  mutate(age_in_months = floor(target_child_age)) %>%
  mutate(speaker_code = if_else(str_detect("CHI", speaker_code), 
                                "CHI", speaker_code))

topic_count_vector <- childes_htmm_pdwz %>%
  group_by(max_topic_15, transcript_id, age_in_months, speaker_role) %>%
  count() %>% ungroup()

counts <- topic_count_vector %>%
  mutate(name = paste0(transcript_id, speaker_role)) 

counts <- split(counts$n, counts$name)

num_topics <- n_distinct(childes_htmm_pdwz$max_topic_15)

```

```{python}
import ndd
k_val = r.num_topics
entropies_dict = {}

for key, value in r.counts.items():
  entropies_dict[key] = ndd.entropy(value, k = k_val)
```

```{r}
topic_entropies <- py$entropies_dict %>%
  as.data.frame() %>% t() %>% as.data.frame() %>%
  rownames_to_column() %>%
  dplyr::rename(name = rowname, entropy = V1) %>%
  mutate(name = str_remove(name, "X"))

topic_entropy <- topic_count_vector %>% 
  mutate(name = paste0(transcript_id, speaker_role)) %>%
  left_join(topic_entropies, by = "name")

par_child_entropies <- topic_entropy %>%
  group_by(transcript_id, speaker_role) %>%
  summarise(entropy = first(entropy), age = first(age_in_months)) %>%
  group_by(age, speaker_role) %>%
  mutate(entropy_random = entropy[sample(row_number())]) %>%
  ungroup()

```

```{r entropy_plot, fig.height = 3.4, fig.width=3.4, fig.align = "center", num.cols.cap=1, fig.cap = "Plot showing entropy of topics produced by children, mothers and fathers over development.", fig.pos="tb"}
plot1<- par_child_entropies %>%
  ggplot(aes(age, entropy, color = speaker_role)) +
  geom_point(alpha = 0.2) +
  geom_smooth() +
  ylab("Entropy") +
  xlab("Age") +
  ggtitle("Entropy of Topics") +
  theme_classic()
```

```{r branching_out_plot}
#t, fig.height = 3.4, fig.width=3.4, fig.align = "center", num.cols.cap=1, fig.cap = "Plot showing the distribution of topics produced by children and mothers across development.", fig.pos="tb"
plot2 <- childes_htmm_pdwz %>%
  filter(speaker_role == "Mother" | speaker_role == "Target_Child") %>%
  ggplot(aes(x = age_in_months, fill = as.factor(max_topic_15))) +
  geom_bar(position = "fill") +
  facet_wrap(~speaker_role) +
  theme_classic() 
```


```{r snippet_table, results="asis", tab.env = "table"}

tab2 <- childes_htmm_pdwz %>%
  filter((transcript_id == 5657 & utterance_order %in% c(64:73)) |
           (transcript_id == 6910 & utterance_order %in% c(4:13)) |
           (transcript_id == 5772 & utterance_order %in% c(678:688)) )%>%
  select(speaker_role, gloss, max_topic_15, age_in_months, transcript_id) %>%
  group_by(age_in_months) %>% mutate(utterance_order = 1:n()) %>% ungroup() %>%
  dplyr::rename(age = age_in_months, topic = max_topic_15) %>%
  arrange(age, utterance_order) %>%
  mutate(age = as.character(age)) %>%
  select(age, speaker_role, topic, gloss) %>%
  xtable()

#transcript_id == 5655 & utterance_order %in% c(691:700)
  #pivot_wider(names_from = age_in_months, values_from = c(speaker_role, gloss, max_topic_15)) %>% 
  #select(contains("29"), contains("38"), contains("43")) 

print(tab2, type = "latex", comment = F, table.placement = "tb", floating = TRUE,
      floating.environment = "table*",
      include.rownames = FALSE)
```
# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent